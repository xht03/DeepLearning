{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b771f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../../Python-Table/task2'))\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import idx2numpy\n",
    "import pandas as pd\n",
    "from lib.Net import *\n",
    "from lib.Func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddb3987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (batch_size, 1, 28, 28) -> (batch_size, 2, 24, 24) -> (batch_size, 2*24*24) -> (batch_size, 64) -> (batch_size, 10)\n",
    "architecture = [\n",
    "    {\"module\": Conv2d, \"params\": {\"in_channels\": 1, \"out_channels\": 2, \"kernel_size\": 5, \"activation\": \"gelu\"}},\n",
    "    {\"module\": Flatten},\n",
    "    {\"module\": Mlp, \"params\": {\"input_dim\": 1152, \"output_dim\": 64, \"activation\": \"gelu\"}},\n",
    "    {\"module\": Dropout, \"params\": {\"p\": 0.3}},\n",
    "    {\"module\": Mlp, \"params\": {\"input_dim\": 64, \"output_dim\": 10, \"activation\": \"softmax\"}},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c462c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578484b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = \"../data/MNIST/train-images.idx3-ubyte\"\n",
    "labelpath = \"../data/MNIST/train-labels.idx1-ubyte\"\n",
    "modelpath = \"../model/task2/MNIST.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a186dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = idx2numpy.convert_from_file(datapath)    # (60000, 28, 28)\n",
    "data = np.expand_dims(data, axis=1)             # 添加通道维度 -> (60000, 1, 28, 28)\n",
    "\n",
    "label = idx2numpy.convert_from_file(labelpath)\n",
    "one_hot_labels = one_hot(label, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b5ae00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/117, Loss: 2.3159\n",
      "Batch 2/117, Loss: 4.1564\n",
      "Batch 3/117, Loss: 4.2462\n",
      "Batch 4/117, Loss: 4.0757\n",
      "Batch 5/117, Loss: 4.0667\n",
      "Batch 6/117, Loss: 4.1116\n",
      "Batch 7/117, Loss: 4.2384\n",
      "Batch 8/117, Loss: 4.1564\n",
      "Batch 9/117, Loss: 4.1385\n",
      "Batch 10/117, Loss: 4.2013\n",
      "Batch 11/117, Loss: 4.1564\n",
      "Batch 12/117, Loss: 4.0308\n",
      "Batch 13/117, Loss: 4.1744\n",
      "Batch 14/117, Loss: 4.0936\n",
      "Batch 15/117, Loss: 4.1564\n",
      "Batch 16/117, Loss: 4.2462\n",
      "Batch 17/117, Loss: 4.2013\n",
      "Batch 18/117, Loss: 4.3000\n",
      "Batch 19/117, Loss: 4.1834\n",
      "Batch 20/117, Loss: 4.2462\n",
      "Batch 21/117, Loss: 4.2103\n",
      "Batch 22/117, Loss: 4.2282\n",
      "Batch 23/117, Loss: 4.2013\n",
      "Batch 24/117, Loss: 4.1654\n",
      "Batch 25/117, Loss: 4.2552\n",
      "Batch 26/117, Loss: 4.1654\n",
      "Batch 27/117, Loss: 4.2911\n",
      "Batch 28/117, Loss: 4.2552\n",
      "Batch 29/117, Loss: 4.2552\n",
      "Batch 30/117, Loss: 4.2372\n",
      "Batch 31/117, Loss: 4.2911\n",
      "Batch 32/117, Loss: 4.2462\n",
      "Batch 33/117, Loss: 4.2193\n",
      "Batch 34/117, Loss: 4.1923\n",
      "Batch 35/117, Loss: 4.1923\n",
      "Batch 36/117, Loss: 4.1385\n",
      "Batch 37/117, Loss: 4.2372\n",
      "Batch 38/117, Loss: 4.2372\n",
      "Batch 39/117, Loss: 4.2731\n",
      "Batch 40/117, Loss: 4.2103\n",
      "Batch 41/117, Loss: 4.2462\n",
      "Batch 42/117, Loss: 4.2462\n",
      "Batch 43/117, Loss: 4.2282\n",
      "Batch 44/117, Loss: 4.2462\n",
      "Batch 45/117, Loss: 4.2731\n",
      "Batch 46/117, Loss: 4.3090\n",
      "Batch 47/117, Loss: 4.2193\n",
      "Batch 48/117, Loss: 4.2731\n",
      "Batch 49/117, Loss: 4.3000\n",
      "Batch 50/117, Loss: 4.2372\n",
      "Batch 51/117, Loss: 4.2462\n",
      "Batch 52/117, Loss: 4.2103\n",
      "Batch 53/117, Loss: 4.2372\n",
      "Batch 54/117, Loss: 4.3000\n",
      "Batch 55/117, Loss: 4.2372\n",
      "Batch 56/117, Loss: 4.0308\n",
      "Batch 57/117, Loss: 4.2552\n",
      "Batch 58/117, Loss: 4.2372\n",
      "Batch 59/117, Loss: 4.2911\n",
      "Batch 60/117, Loss: 4.1923\n",
      "Batch 61/117, Loss: 4.2462\n",
      "Batch 62/117, Loss: 4.2372\n",
      "Batch 63/117, Loss: 4.2731\n",
      "Batch 64/117, Loss: 4.2282\n",
      "Batch 65/117, Loss: 4.2372\n",
      "Batch 66/117, Loss: 4.2552\n",
      "Batch 67/117, Loss: 4.3359\n",
      "Batch 68/117, Loss: 4.2731\n",
      "Batch 69/117, Loss: 4.2462\n",
      "Batch 70/117, Loss: 4.1385\n",
      "Batch 71/117, Loss: 4.2552\n",
      "Batch 72/117, Loss: 4.3000\n",
      "Batch 73/117, Loss: 4.1295\n",
      "Batch 74/117, Loss: 4.0757\n",
      "Batch 75/117, Loss: 4.0846\n",
      "Batch 76/117, Loss: 4.0936\n",
      "Batch 77/117, Loss: 4.2462\n",
      "Batch 78/117, Loss: 4.1116\n",
      "Batch 79/117, Loss: 4.2552\n",
      "Batch 80/117, Loss: 4.1744\n",
      "Batch 81/117, Loss: 4.1564\n",
      "Batch 82/117, Loss: 4.1385\n",
      "Batch 83/117, Loss: 4.2372\n",
      "Batch 84/117, Loss: 4.1654\n",
      "Batch 85/117, Loss: 4.2193\n",
      "Batch 86/117, Loss: 4.1205\n",
      "Batch 87/117, Loss: 4.3090\n",
      "Batch 88/117, Loss: 4.0936\n",
      "Batch 89/117, Loss: 4.1654\n",
      "Batch 90/117, Loss: 4.2193\n",
      "Batch 91/117, Loss: 4.2641\n",
      "Batch 92/117, Loss: 4.2013\n",
      "Batch 93/117, Loss: 4.2911\n",
      "Batch 94/117, Loss: 4.1654\n",
      "Batch 95/117, Loss: 4.1385\n",
      "Batch 96/117, Loss: 4.3180\n",
      "Batch 97/117, Loss: 4.1654\n",
      "Batch 98/117, Loss: 4.0667\n",
      "Batch 99/117, Loss: 4.0846\n",
      "Batch 100/117, Loss: 4.2103\n",
      "Batch 101/117, Loss: 4.0308\n",
      "Batch 102/117, Loss: 4.1475\n",
      "Batch 103/117, Loss: 4.1205\n",
      "Batch 104/117, Loss: 4.1564\n",
      "Batch 105/117, Loss: 4.2013\n",
      "Batch 106/117, Loss: 4.1295\n",
      "Batch 107/117, Loss: 4.1475\n",
      "Batch 108/117, Loss: 4.1923\n",
      "Batch 109/117, Loss: 4.1654\n",
      "Batch 110/117, Loss: 4.2193\n",
      "Batch 111/117, Loss: 4.1744\n",
      "Batch 112/117, Loss: 4.2282\n",
      "Batch 113/117, Loss: 4.0308\n",
      "Batch 114/117, Loss: 4.0667\n",
      "Batch 115/117, Loss: 4.1116\n",
      "Batch 116/117, Loss: 4.1205\n",
      "Batch 117/117, Loss: 4.1116\n",
      "Batch 118/117, Loss: 4.1265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:47<07:09, 47.71s/it, loss=4.16, accuracy=9.75%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/117, Loss: 4.1564\n",
      "Batch 2/117, Loss: 4.1834\n",
      "Batch 3/117, Loss: 4.1564\n",
      "Batch 4/117, Loss: 4.1834\n",
      "Batch 5/117, Loss: 4.2013\n",
      "Batch 6/117, Loss: 4.1475\n",
      "Batch 7/117, Loss: 4.1834\n",
      "Batch 8/117, Loss: 4.1205\n",
      "Batch 9/117, Loss: 4.0577\n",
      "Batch 10/117, Loss: 4.0308\n",
      "Batch 11/117, Loss: 4.1026\n",
      "Batch 12/117, Loss: 4.2462\n",
      "Batch 13/117, Loss: 4.2013\n",
      "Batch 14/117, Loss: 4.2103\n",
      "Batch 15/117, Loss: 4.2282\n",
      "Batch 16/117, Loss: 4.2282\n",
      "Batch 17/117, Loss: 4.1834\n",
      "Batch 18/117, Loss: 4.2372\n",
      "Batch 19/117, Loss: 4.1295\n",
      "Batch 20/117, Loss: 4.1295\n",
      "Batch 21/117, Loss: 4.0398\n",
      "Batch 22/117, Loss: 4.2282\n",
      "Batch 23/117, Loss: 4.1295\n",
      "Batch 24/117, Loss: 4.1654\n",
      "Batch 25/117, Loss: 4.1744\n",
      "Batch 26/117, Loss: 4.1475\n",
      "Batch 27/117, Loss: 4.1744\n",
      "Batch 28/117, Loss: 4.1834\n",
      "Batch 29/117, Loss: 4.2641\n",
      "Batch 30/117, Loss: 4.1744\n",
      "Batch 31/117, Loss: 4.2103\n",
      "Batch 32/117, Loss: 4.2372\n",
      "Batch 33/117, Loss: 4.1654\n",
      "Batch 34/117, Loss: 4.2552\n",
      "Batch 35/117, Loss: 4.1564\n",
      "Batch 36/117, Loss: 4.2193\n",
      "Batch 37/117, Loss: 4.0487\n",
      "Batch 38/117, Loss: 4.2282\n",
      "Batch 39/117, Loss: 4.1295\n",
      "Batch 40/117, Loss: 4.2193\n",
      "Batch 41/117, Loss: 4.1564\n",
      "Batch 42/117, Loss: 4.2552\n",
      "Batch 43/117, Loss: 4.1295\n",
      "Batch 44/117, Loss: 4.1475\n",
      "Batch 45/117, Loss: 4.1295\n",
      "Batch 46/117, Loss: 4.2193\n",
      "Batch 47/117, Loss: 4.1923\n",
      "Batch 48/117, Loss: 4.2013\n",
      "Batch 49/117, Loss: 4.1385\n",
      "Batch 50/117, Loss: 4.2641\n",
      "Batch 51/117, Loss: 4.1295\n",
      "Batch 52/117, Loss: 4.1205\n",
      "Batch 53/117, Loss: 4.0846\n",
      "Batch 54/117, Loss: 4.1116\n",
      "Batch 55/117, Loss: 4.1205\n",
      "Batch 56/117, Loss: 4.1564\n",
      "Batch 57/117, Loss: 4.0487\n",
      "Batch 58/117, Loss: 4.1116\n",
      "Batch 59/117, Loss: 4.1834\n",
      "Batch 60/117, Loss: 4.1295\n",
      "Batch 61/117, Loss: 4.1564\n",
      "Batch 62/117, Loss: 4.1385\n",
      "Batch 63/117, Loss: 4.1116\n",
      "Batch 64/117, Loss: 4.1834\n",
      "Batch 65/117, Loss: 4.2462\n",
      "Batch 66/117, Loss: 4.1475\n",
      "Batch 67/117, Loss: 4.1205\n",
      "Batch 68/117, Loss: 4.2013\n",
      "Batch 69/117, Loss: 4.1475\n",
      "Batch 70/117, Loss: 4.1744\n",
      "Batch 71/117, Loss: 4.0757\n",
      "Batch 72/117, Loss: 4.0846\n",
      "Batch 73/117, Loss: 4.1385\n",
      "Batch 74/117, Loss: 4.1385\n",
      "Batch 75/117, Loss: 4.2103\n",
      "Batch 76/117, Loss: 4.1654\n",
      "Batch 77/117, Loss: 4.1834\n",
      "Batch 78/117, Loss: 4.2372\n",
      "Batch 79/117, Loss: 4.1475\n",
      "Batch 80/117, Loss: 4.2013\n",
      "Batch 81/117, Loss: 4.1385\n",
      "Batch 82/117, Loss: 4.0757\n",
      "Batch 83/117, Loss: 4.1205\n",
      "Batch 84/117, Loss: 4.0667\n",
      "Batch 85/117, Loss: 4.0936\n",
      "Batch 86/117, Loss: 4.0667\n",
      "Batch 87/117, Loss: 4.1564\n",
      "Batch 88/117, Loss: 4.1026\n",
      "Batch 89/117, Loss: 4.0936\n",
      "Batch 90/117, Loss: 4.1385\n",
      "Batch 91/117, Loss: 4.1205\n",
      "Batch 92/117, Loss: 4.2552\n",
      "Batch 93/117, Loss: 4.2372\n",
      "Batch 94/117, Loss: 4.0936\n",
      "Batch 95/117, Loss: 4.2013\n",
      "Batch 96/117, Loss: 4.1295\n",
      "Batch 97/117, Loss: 4.0757\n",
      "Batch 98/117, Loss: 4.2013\n",
      "Batch 99/117, Loss: 4.2013\n",
      "Batch 100/117, Loss: 4.1116\n",
      "Batch 101/117, Loss: 4.1744\n",
      "Batch 102/117, Loss: 4.1385\n",
      "Batch 103/117, Loss: 4.1385\n",
      "Batch 104/117, Loss: 4.0936\n",
      "Batch 105/117, Loss: 4.0936\n",
      "Batch 106/117, Loss: 4.1205\n",
      "Batch 107/117, Loss: 4.1923\n",
      "Batch 108/117, Loss: 4.1654\n",
      "Batch 109/117, Loss: 4.1385\n",
      "Batch 110/117, Loss: 4.0936\n",
      "Batch 111/117, Loss: 4.2013\n",
      "Batch 112/117, Loss: 4.1295\n",
      "Batch 113/117, Loss: 4.2372\n",
      "Batch 114/117, Loss: 4.1654\n",
      "Batch 115/117, Loss: 4.2282\n",
      "Batch 116/117, Loss: 4.1385\n",
      "Batch 117/117, Loss: 4.1295\n",
      "Batch 118/117, Loss: 4.2222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:36<06:28, 48.54s/it, loss=4.16, accuracy=9.75%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/117, Loss: 4.2013\n",
      "Batch 2/117, Loss: 4.1834\n",
      "Batch 3/117, Loss: 4.2821\n",
      "Batch 4/117, Loss: 4.1564\n",
      "Batch 5/117, Loss: 4.1564\n",
      "Batch 6/117, Loss: 4.1475\n",
      "Batch 7/117, Loss: 4.2282\n",
      "Batch 8/117, Loss: 4.1564\n",
      "Batch 9/117, Loss: 4.1834\n",
      "Batch 10/117, Loss: 4.0936\n",
      "Batch 11/117, Loss: 4.0846\n",
      "Batch 12/117, Loss: 4.0936\n",
      "Batch 13/117, Loss: 4.1744\n",
      "Batch 14/117, Loss: 4.1744\n",
      "Batch 15/117, Loss: 4.1744\n",
      "Batch 16/117, Loss: 4.0577\n",
      "Batch 17/117, Loss: 4.1654\n",
      "Batch 18/117, Loss: 4.1026\n",
      "Batch 19/117, Loss: 4.1385\n",
      "Batch 20/117, Loss: 4.1564\n",
      "Batch 21/117, Loss: 4.1385\n",
      "Batch 22/117, Loss: 4.0757\n",
      "Batch 23/117, Loss: 4.2641\n",
      "Batch 24/117, Loss: 4.2103\n",
      "Batch 25/117, Loss: 4.0487\n",
      "Batch 26/117, Loss: 4.1923\n",
      "Batch 27/117, Loss: 4.1744\n",
      "Batch 28/117, Loss: 4.2282\n",
      "Batch 29/117, Loss: 4.0667\n",
      "Batch 30/117, Loss: 4.1744\n",
      "Batch 31/117, Loss: 4.1026\n",
      "Batch 32/117, Loss: 4.2103\n",
      "Batch 33/117, Loss: 4.1834\n",
      "Batch 34/117, Loss: 4.2282\n",
      "Batch 35/117, Loss: 4.2641\n",
      "Batch 36/117, Loss: 4.1744\n",
      "Batch 37/117, Loss: 4.1564\n",
      "Batch 38/117, Loss: 4.1654\n",
      "Batch 39/117, Loss: 4.2821\n",
      "Batch 40/117, Loss: 4.2193\n",
      "Batch 41/117, Loss: 4.1026\n",
      "Batch 42/117, Loss: 4.1295\n",
      "Batch 43/117, Loss: 4.1654\n",
      "Batch 44/117, Loss: 4.1654\n",
      "Batch 45/117, Loss: 4.1205\n",
      "Batch 46/117, Loss: 4.0936\n",
      "Batch 47/117, Loss: 4.2013\n",
      "Batch 48/117, Loss: 4.1385\n",
      "Batch 49/117, Loss: 4.2013\n",
      "Batch 50/117, Loss: 4.1654\n",
      "Batch 51/117, Loss: 4.0936\n",
      "Batch 52/117, Loss: 4.1564\n",
      "Batch 53/117, Loss: 4.1564\n",
      "Batch 54/117, Loss: 4.0757\n",
      "Batch 55/117, Loss: 4.1654\n",
      "Batch 56/117, Loss: 4.1834\n",
      "Batch 57/117, Loss: 4.1385\n",
      "Batch 58/117, Loss: 4.1564\n",
      "Batch 59/117, Loss: 4.2731\n",
      "Batch 60/117, Loss: 4.1564\n",
      "Batch 61/117, Loss: 4.1834\n",
      "Batch 62/117, Loss: 4.1116\n",
      "Batch 63/117, Loss: 4.1834\n",
      "Batch 64/117, Loss: 4.0757\n",
      "Batch 65/117, Loss: 4.1564\n",
      "Batch 66/117, Loss: 4.1385\n",
      "Batch 67/117, Loss: 4.0218\n",
      "Batch 68/117, Loss: 4.1923\n",
      "Batch 69/117, Loss: 4.2372\n",
      "Batch 70/117, Loss: 4.1834\n",
      "Batch 71/117, Loss: 4.1923\n",
      "Batch 72/117, Loss: 4.1026\n",
      "Batch 73/117, Loss: 4.1744\n",
      "Batch 74/117, Loss: 4.0577\n",
      "Batch 75/117, Loss: 4.1475\n",
      "Batch 76/117, Loss: 4.0936\n",
      "Batch 77/117, Loss: 4.1385\n",
      "Batch 78/117, Loss: 4.1744\n",
      "Batch 79/117, Loss: 4.2372\n",
      "Batch 80/117, Loss: 4.0577\n",
      "Batch 81/117, Loss: 4.0846\n",
      "Batch 82/117, Loss: 4.1654\n",
      "Batch 83/117, Loss: 4.0128\n",
      "Batch 84/117, Loss: 4.1205\n",
      "Batch 85/117, Loss: 4.0577\n",
      "Batch 86/117, Loss: 4.1564\n",
      "Batch 87/117, Loss: 4.2013\n",
      "Batch 88/117, Loss: 4.2103\n",
      "Batch 89/117, Loss: 4.1295\n",
      "Batch 90/117, Loss: 4.2103\n",
      "Batch 91/117, Loss: 4.1744\n",
      "Batch 92/117, Loss: 4.2282\n",
      "Batch 93/117, Loss: 4.1295\n",
      "Batch 94/117, Loss: 4.0846\n",
      "Batch 95/117, Loss: 4.1834\n",
      "Batch 96/117, Loss: 4.2013\n",
      "Batch 97/117, Loss: 4.1385\n",
      "Batch 98/117, Loss: 4.1475\n",
      "Batch 99/117, Loss: 4.2103\n",
      "Batch 100/117, Loss: 4.1564\n",
      "Batch 101/117, Loss: 4.0577\n",
      "Batch 102/117, Loss: 4.1475\n",
      "Batch 103/117, Loss: 4.0936\n",
      "Batch 104/117, Loss: 4.2013\n",
      "Batch 105/117, Loss: 4.2103\n",
      "Batch 106/117, Loss: 4.1744\n",
      "Batch 107/117, Loss: 4.0577\n",
      "Batch 108/117, Loss: 4.2013\n",
      "Batch 109/117, Loss: 4.2552\n",
      "Batch 110/117, Loss: 4.1654\n",
      "Batch 111/117, Loss: 4.1205\n",
      "Batch 112/117, Loss: 4.1744\n",
      "Batch 113/117, Loss: 4.2193\n",
      "Batch 114/117, Loss: 4.1564\n",
      "Batch 115/117, Loss: 4.2013\n",
      "Batch 116/117, Loss: 4.2731\n",
      "Batch 117/117, Loss: 4.1026\n",
      "Batch 118/117, Loss: 4.1265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [02:25<05:40, 48.63s/it, loss=4.16, accuracy=9.75%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/117, Loss: 4.1834\n",
      "Batch 2/117, Loss: 4.2103\n",
      "Batch 3/117, Loss: 4.1205\n",
      "Batch 4/117, Loss: 4.1385\n",
      "Batch 5/117, Loss: 4.1834\n",
      "Batch 6/117, Loss: 4.2193\n",
      "Batch 7/117, Loss: 4.1205\n",
      "Batch 8/117, Loss: 4.2282\n",
      "Batch 9/117, Loss: 4.1205\n",
      "Batch 10/117, Loss: 4.1026\n",
      "Batch 11/117, Loss: 4.1295\n",
      "Batch 12/117, Loss: 4.1834\n",
      "Batch 13/117, Loss: 4.0577\n",
      "Batch 14/117, Loss: 4.1744\n",
      "Batch 15/117, Loss: 4.1026\n",
      "Batch 16/117, Loss: 4.1116\n",
      "Batch 17/117, Loss: 4.2462\n",
      "Batch 18/117, Loss: 4.2911\n",
      "Batch 19/117, Loss: 4.1923\n",
      "Batch 20/117, Loss: 4.2641\n",
      "Batch 21/117, Loss: 4.2641\n",
      "Batch 22/117, Loss: 4.1923\n",
      "Batch 23/117, Loss: 4.2552\n",
      "Batch 24/117, Loss: 4.1834\n",
      "Batch 25/117, Loss: 4.2641\n",
      "Batch 26/117, Loss: 4.0039\n",
      "Batch 27/117, Loss: 4.1744\n",
      "Batch 28/117, Loss: 4.1744\n",
      "Batch 29/117, Loss: 4.0667\n",
      "Batch 30/117, Loss: 4.0846\n",
      "Batch 31/117, Loss: 4.1026\n",
      "Batch 32/117, Loss: 4.1385\n",
      "Batch 33/117, Loss: 4.2013\n",
      "Batch 34/117, Loss: 4.1385\n",
      "Batch 35/117, Loss: 4.1834\n",
      "Batch 36/117, Loss: 4.1744\n",
      "Batch 37/117, Loss: 4.2193\n",
      "Batch 38/117, Loss: 4.0936\n",
      "Batch 39/117, Loss: 4.0936\n",
      "Batch 40/117, Loss: 4.1385\n",
      "Batch 41/117, Loss: 4.1475\n",
      "Batch 42/117, Loss: 4.2372\n",
      "Batch 43/117, Loss: 4.1205\n",
      "Batch 44/117, Loss: 4.1564\n",
      "Batch 45/117, Loss: 4.1026\n",
      "Batch 46/117, Loss: 4.2103\n",
      "Batch 47/117, Loss: 4.1834\n",
      "Batch 48/117, Loss: 4.1923\n",
      "Batch 49/117, Loss: 4.1923\n",
      "Batch 50/117, Loss: 4.2821\n",
      "Batch 51/117, Loss: 4.2731\n",
      "Batch 52/117, Loss: 4.1654\n",
      "Batch 53/117, Loss: 4.2103\n",
      "Batch 54/117, Loss: 4.1295\n",
      "Batch 55/117, Loss: 4.2821\n",
      "Batch 56/117, Loss: 4.0936\n",
      "Batch 57/117, Loss: 4.1923\n",
      "Batch 58/117, Loss: 4.1744\n",
      "Batch 59/117, Loss: 4.1295\n",
      "Batch 60/117, Loss: 4.1744\n",
      "Batch 61/117, Loss: 4.1295\n",
      "Batch 62/117, Loss: 4.2731\n",
      "Batch 63/117, Loss: 4.2372\n",
      "Batch 64/117, Loss: 4.1564\n",
      "Batch 65/117, Loss: 4.0936\n",
      "Batch 66/117, Loss: 4.0936\n",
      "Batch 67/117, Loss: 4.0308\n",
      "Batch 68/117, Loss: 4.1564\n",
      "Batch 69/117, Loss: 4.2372\n",
      "Batch 70/117, Loss: 4.2013\n",
      "Batch 71/117, Loss: 4.1564\n",
      "Batch 72/117, Loss: 4.0667\n",
      "Batch 73/117, Loss: 4.1744\n",
      "Batch 74/117, Loss: 4.0667\n",
      "Batch 75/117, Loss: 4.2282\n",
      "Batch 76/117, Loss: 4.0936\n",
      "Batch 77/117, Loss: 4.2013\n",
      "Batch 78/117, Loss: 4.1564\n",
      "Batch 79/117, Loss: 4.0846\n",
      "Batch 80/117, Loss: 4.1564\n",
      "Batch 81/117, Loss: 4.2193\n",
      "Batch 82/117, Loss: 4.0577\n",
      "Batch 83/117, Loss: 4.0487\n",
      "Batch 84/117, Loss: 4.1205\n",
      "Batch 85/117, Loss: 4.0577\n",
      "Batch 86/117, Loss: 4.1026\n",
      "Batch 87/117, Loss: 4.2282\n",
      "Batch 88/117, Loss: 4.1834\n",
      "Batch 89/117, Loss: 4.1564\n",
      "Batch 90/117, Loss: 4.2013\n",
      "Batch 91/117, Loss: 4.0667\n",
      "Batch 92/117, Loss: 4.0308\n",
      "Batch 93/117, Loss: 4.1116\n",
      "Batch 94/117, Loss: 4.0757\n",
      "Batch 95/117, Loss: 4.1385\n",
      "Batch 96/117, Loss: 4.1564\n",
      "Batch 97/117, Loss: 4.0936\n",
      "Batch 98/117, Loss: 4.2103\n",
      "Batch 99/117, Loss: 4.1116\n",
      "Batch 100/117, Loss: 4.1744\n",
      "Batch 101/117, Loss: 4.1026\n",
      "Batch 102/117, Loss: 4.2193\n",
      "Batch 103/117, Loss: 4.1923\n",
      "Batch 104/117, Loss: 4.1654\n",
      "Batch 105/117, Loss: 4.0577\n",
      "Batch 106/117, Loss: 4.1654\n",
      "Batch 107/117, Loss: 4.1564\n",
      "Batch 108/117, Loss: 4.2013\n",
      "Batch 109/117, Loss: 4.2103\n",
      "Batch 110/117, Loss: 4.1475\n",
      "Batch 111/117, Loss: 4.0936\n",
      "Batch 112/117, Loss: 4.0757\n",
      "Batch 113/117, Loss: 4.1654\n",
      "Batch 114/117, Loss: 4.2462\n",
      "Batch 115/117, Loss: 4.2193\n",
      "Batch 116/117, Loss: 4.1923\n",
      "Batch 117/117, Loss: 4.1205\n",
      "Batch 118/117, Loss: 3.7915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [03:13<04:49, 48.32s/it, loss=4.16, accuracy=9.75%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1/117, Loss: 4.1834\n",
      "Batch 2/117, Loss: 4.2731\n",
      "Batch 3/117, Loss: 4.2462\n",
      "Batch 4/117, Loss: 4.1654\n",
      "Batch 5/117, Loss: 4.0936\n",
      "Batch 6/117, Loss: 4.1026\n",
      "Batch 7/117, Loss: 4.2013\n",
      "Batch 8/117, Loss: 4.1923\n",
      "Batch 9/117, Loss: 4.1654\n",
      "Batch 10/117, Loss: 4.1205\n",
      "Batch 11/117, Loss: 4.1385\n",
      "Batch 12/117, Loss: 4.1026\n",
      "Batch 13/117, Loss: 4.2462\n",
      "Batch 14/117, Loss: 4.1026\n",
      "Batch 15/117, Loss: 4.2103\n",
      "Batch 16/117, Loss: 4.2372\n",
      "Batch 17/117, Loss: 4.1116\n",
      "Batch 18/117, Loss: 4.1654\n",
      "Batch 19/117, Loss: 4.2282\n",
      "Batch 20/117, Loss: 4.2372\n",
      "Batch 21/117, Loss: 4.1116\n",
      "Batch 22/117, Loss: 4.1475\n",
      "Batch 23/117, Loss: 3.9859\n",
      "Batch 24/117, Loss: 4.1564\n",
      "Batch 25/117, Loss: 4.2641\n",
      "Batch 26/117, Loss: 4.1654\n",
      "Batch 27/117, Loss: 4.2462\n",
      "Batch 28/117, Loss: 4.2731\n",
      "Batch 29/117, Loss: 4.2103\n",
      "Batch 30/117, Loss: 4.0577\n",
      "Batch 31/117, Loss: 4.2911\n",
      "Batch 32/117, Loss: 4.1475\n",
      "Batch 33/117, Loss: 4.1564\n",
      "Batch 34/117, Loss: 4.1295\n",
      "Batch 35/117, Loss: 4.1564\n",
      "Batch 36/117, Loss: 4.1295\n",
      "Batch 37/117, Loss: 4.0128\n",
      "Batch 38/117, Loss: 4.1923\n",
      "Batch 39/117, Loss: 4.2372\n",
      "Batch 40/117, Loss: 4.2282\n",
      "Batch 41/117, Loss: 4.1295\n",
      "Batch 42/117, Loss: 4.0757\n",
      "Batch 43/117, Loss: 4.1295\n",
      "Batch 44/117, Loss: 4.1564\n",
      "Batch 45/117, Loss: 4.2013\n",
      "Batch 46/117, Loss: 4.1026\n",
      "Batch 47/117, Loss: 4.2462\n",
      "Batch 48/117, Loss: 4.0308\n",
      "Batch 49/117, Loss: 4.1475\n",
      "Batch 50/117, Loss: 4.2103\n",
      "Batch 51/117, Loss: 4.1923\n",
      "Batch 52/117, Loss: 4.2372\n",
      "Batch 53/117, Loss: 4.1295\n",
      "Batch 54/117, Loss: 4.2013\n",
      "Batch 55/117, Loss: 4.2013\n",
      "Batch 56/117, Loss: 4.1654\n",
      "Batch 57/117, Loss: 4.3270\n",
      "Batch 58/117, Loss: 4.1385\n",
      "Batch 59/117, Loss: 4.1295\n",
      "Batch 60/117, Loss: 4.2103\n",
      "Batch 61/117, Loss: 4.0218\n",
      "Batch 62/117, Loss: 4.1923\n",
      "Batch 63/117, Loss: 4.1026\n",
      "Batch 64/117, Loss: 4.1475\n",
      "Batch 65/117, Loss: 4.1205\n",
      "Batch 66/117, Loss: 4.1654\n",
      "Batch 67/117, Loss: 4.1744\n",
      "Batch 68/117, Loss: 4.0487\n",
      "Batch 69/117, Loss: 4.1026\n",
      "Batch 70/117, Loss: 4.1654\n",
      "Batch 71/117, Loss: 3.9949\n",
      "Batch 72/117, Loss: 4.1834\n",
      "Batch 73/117, Loss: 4.0936\n",
      "Batch 74/117, Loss: 4.1744\n",
      "Batch 75/117, Loss: 4.0398\n",
      "Batch 76/117, Loss: 4.2103\n",
      "Batch 77/117, Loss: 4.1295\n",
      "Batch 78/117, Loss: 4.0846\n",
      "Batch 79/117, Loss: 4.2193\n",
      "Batch 80/117, Loss: 4.0757\n",
      "Batch 81/117, Loss: 4.0577\n",
      "Batch 82/117, Loss: 4.1834\n",
      "Batch 83/117, Loss: 3.9949\n",
      "Batch 84/117, Loss: 4.0667\n",
      "Batch 85/117, Loss: 4.2282\n",
      "Batch 86/117, Loss: 4.0667\n",
      "Batch 87/117, Loss: 4.1475\n",
      "Batch 88/117, Loss: 4.1116\n",
      "Batch 89/117, Loss: 4.2193\n",
      "Batch 90/117, Loss: 4.2103\n",
      "Batch 91/117, Loss: 4.1834\n",
      "Batch 92/117, Loss: 4.1026\n",
      "Batch 93/117, Loss: 4.1923\n",
      "Batch 94/117, Loss: 4.1654\n",
      "Batch 95/117, Loss: 4.1744\n",
      "Batch 96/117, Loss: 4.1295\n",
      "Batch 97/117, Loss: 4.0846\n",
      "Batch 98/117, Loss: 4.1744\n",
      "Batch 99/117, Loss: 4.0757\n",
      "Batch 100/117, Loss: 4.1564\n",
      "Batch 101/117, Loss: 4.2013\n",
      "Batch 102/117, Loss: 4.3000\n",
      "Batch 103/117, Loss: 4.2731\n",
      "Batch 104/117, Loss: 4.1923\n",
      "Batch 105/117, Loss: 4.1116\n",
      "Batch 106/117, Loss: 4.2552\n",
      "Batch 107/117, Loss: 4.3000\n",
      "Batch 108/117, Loss: 4.1026\n",
      "Batch 109/117, Loss: 4.0577\n",
      "Batch 110/117, Loss: 4.1654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [03:48<05:42, 57.08s/it, loss=4.16, accuracy=9.75%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[32m      5\u001b[39m     data, one_hot_labels = shuffle(data, one_hot_labels)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43mnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_hot_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlossfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcross_entropy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     y_hat = net.predict(data)\n\u001b[32m      9\u001b[39m     loss = cross_entropy_loss(one_hot_labels, y_hat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI/PJ1/lib/Net.py:100\u001b[39m, in \u001b[36mNet.train\u001b[39m\u001b[34m(self, X, Y, batch_size, lr, lossfunc)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;66;03m# 反向传播\u001b[39;00m\n\u001b[32m     99\u001b[39m dL = \u001b[38;5;28mself\u001b[39m.loss_derivative(Y_batch, y_hat)\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m//\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X)\u001b[38;5;250m \u001b[39m//\u001b[38;5;250m \u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.loss(Y_batch,\u001b[38;5;250m \u001b[39my_hat)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI/PJ1/lib/Net.py:74\u001b[39m, in \u001b[36mNet.backward\u001b[39m\u001b[34m(self, outputs, dL, lr)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, outputs, dL, lr):\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.net) - \u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m         dL = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AI/PJ1/lib/Net.py:291\u001b[39m, in \u001b[36mConv2d.backward\u001b[39m\u001b[34m(self, X, dL, lr)\u001b[39m\n\u001b[32m    289\u001b[39m         X_region = X[:, :, h : h + height_out, w : w + width_out]\n\u001b[32m    290\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.out_channels):\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m             dW[k, :, h, w] = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_region\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mdZ\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m /batch_size\n\u001b[32m    292\u001b[39m \u001b[38;5;66;03m# ---------------------------------------\u001b[39;00m\n\u001b[32m    293\u001b[39m db = np.sum(dZ, axis=(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m)) / batch_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:2333\u001b[39m, in \u001b[36m_sum_dispatcher\u001b[39m\u001b[34m(a, axis, dtype, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2327\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPassing `min` or `max` keyword argument when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2328\u001b[39m                          \u001b[33m\"\u001b[39m\u001b[33m`a_min` and `a_max` are provided is forbidden.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2330\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapfunc(a, \u001b[33m'\u001b[39m\u001b[33mclip\u001b[39m\u001b[33m'\u001b[39m, a_min, a_max, out=out, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m_sum_dispatcher\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   2334\u001b[39m                     initial=\u001b[38;5;28;01mNone\u001b[39;00m, where=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2335\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, out)\n\u001b[32m   2338\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_sum_dispatcher)\n\u001b[32m   2339\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34msum\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue,\n\u001b[32m   2340\u001b[39m         initial=np._NoValue, where=np._NoValue):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "pbar =tqdm(range(epochs))\n",
    "\n",
    "for i in pbar:\n",
    "    data, one_hot_labels = shuffle(data, one_hot_labels)\n",
    "    net.train(data, one_hot_labels, batch_size=512, lr=0.1, lossfunc=\"cross_entropy\")\n",
    "\n",
    "    y_hat = net.predict(data)\n",
    "    loss = cross_entropy_loss(one_hot_labels, y_hat)\n",
    "    y_hat = np.argmax(y_hat, axis=1)    # (60000, 10) -> (60000,)\n",
    "    accuracy = np.mean(y_hat == one_hot_labels.argmax(axis=1))\n",
    "    pbar.set_postfix({\"loss\": loss, \"accuracy\": f\"{accuracy*100:.2f}%\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96f978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.save_params(modelpath_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199311a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
