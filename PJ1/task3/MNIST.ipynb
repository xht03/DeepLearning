{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d9b53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import idx2numpy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a33a56b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5120a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差块\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # 如果输入输出维度不匹配，使用1x1卷积调整\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5dfd879",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            ResidualBlock(32, 32),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            ResidualBlock(32, 64),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc5b6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, num_classes):\n",
    "    # one-hot 编码\n",
    "    one_hot_labels = np.zeros((labels.shape[0], num_classes))\n",
    "    for i in range(labels.shape[0]):\n",
    "        one_hot_labels[i, labels[i]] = 1\n",
    "    return one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0691d41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1899/3395580600.py:7: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)\n",
      "  data = torch.from_numpy(data).float()\n"
     ]
    }
   ],
   "source": [
    "datapath = \"../data/MNIST/train-images.idx3-ubyte\"\n",
    "labelpath = \"../data/MNIST/train-labels.idx1-ubyte\"\n",
    "modelpath = \"../model/task3/MNIST.pth\"\n",
    "\n",
    "data = idx2numpy.convert_from_file(datapath)    # (60000, 28, 28)\n",
    "data = np.expand_dims(data, axis=1)             # 添加通道维度 -> (60000, 1, 28, 28)\n",
    "data = torch.from_numpy(data).float()\n",
    "\n",
    "label = idx2numpy.convert_from_file(labelpath)\n",
    "one_hot_labels = one_hot(label, 10)\n",
    "one_hot_labels = torch.from_numpy(one_hot_labels).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e2eb343",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datapath = \"../valid/MNIST/t10k-images.idx3-ubyte\"\n",
    "valid_labelpath = \"../valid/MNIST/t10k-labels.idx1-ubyte\"\n",
    "\n",
    "valid_data = idx2numpy.convert_from_file(valid_datapath)\n",
    "valid_data = np.expand_dims(valid_data, axis=1)\n",
    "valid_data = torch.from_numpy(valid_data).float()\n",
    "\n",
    "valid_label = idx2numpy.convert_from_file(valid_labelpath)\n",
    "valid_one_hot_labels = one_hot(valid_label, 10)\n",
    "valid_one_hot_labels = torch.from_numpy(valid_one_hot_labels).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fa37655",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(data, one_hot_labels)\n",
    "valid_dataset = TensorDataset(valid_data, valid_one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72d7bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50            # 训练轮数\n",
    "\n",
    "batch_size = 128        # 批大小\n",
    "inital_lr = 0.001       # 初始学习率\n",
    "lr_patience = 10        # 学习率衰减的耐心\n",
    "lr_decay = 0.5          # 学习率衰减系数\n",
    "\n",
    "best_accuracy = 0.0     # 最佳准确率\n",
    "best_loss = float(\"inf\")  # 最佳损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c97a21b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size, True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size, False)\n",
    "\n",
    "model = MNIST().to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=inital_lr, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_decay, patience=lr_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db68e2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [01:39<00:00,  2.00s/it, accuracy=99.60%, best_accuracy=99.54%, best_loss=0.0183, loss=0.00142, lr=0.00025, valid_loss=0.0307]\n"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(range(epochs), desc=\"Training\")\n",
    "for i in pbar:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    for x, y in train_loader:\n",
    "        # 加载进GPU\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        # 清空梯度\n",
    "        optimizer.zero_grad()\n",
    "        # 前向传播\n",
    "        output = model(x)\n",
    "        # 计算损失\n",
    "        loss = loss_func(output, y)\n",
    "        running_loss += loss.item()\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "\n",
    "    # 计算验证集损失和准确率\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # 预测\n",
    "            pred = model(x)\n",
    "            # 计算损失\n",
    "            loss = loss_func(pred, y)\n",
    "            valid_loss += loss.item()\n",
    "            # 计算准确率\n",
    "            accuracy += torch.sum(torch.argmax(pred, dim=1) == torch.argmax(y, dim=1)).item()\n",
    "\n",
    "    running_loss /= len(train_loader)\n",
    "    valid_loss /= len(valid_loader)\n",
    "    accuracy /= len(valid_loader.dataset)\n",
    "    scheduler.step(valid_loss)  # 更新学习率\n",
    "\n",
    "    # if accuracy > best_accuracy:\n",
    "    #     best_accuracy = accuracy\n",
    "    #     torch.save(model.state_dict(), modelpath)\n",
    "\n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        best_accuracy = accuracy\n",
    "        torch.save(model.state_dict(), modelpath)\n",
    "\n",
    "    pbar.set_postfix(\n",
    "        loss=running_loss,\n",
    "        valid_loss=valid_loss,\n",
    "        best_loss=best_loss,\n",
    "        accuracy=f\"{accuracy*100:.2f}%\",\n",
    "        best_accuracy=f\"{best_accuracy*100:.2f}%\",\n",
    "        lr=optimizer.param_groups[0]['lr'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3574cc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture: MNIST(\n",
      "  (net): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential()\n",
      "    )\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ResidualBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (shortcut): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Flatten(start_dim=1, end_dim=-1)\n",
      "    (8): Linear(in_features=3136, out_features=128, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): Dropout(p=0.5, inplace=False)\n",
      "    (11): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "param 479498\n",
      "savepath: ../model/task3/MNIST.pth\n",
      "best_accuracy: 0.9954\n"
     ]
    }
   ],
   "source": [
    "print(\"architecture:\", model)\n",
    "print(\"param\", sum(p.numel() for p in model.parameters()))\n",
    "print(\"savepath:\", modelpath)\n",
    "print(\"best_accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "614bc522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For interview\n",
    "interview_data_path = \"../test/t10k-images.idx3-ubyte\"\n",
    "interview_label_path = \"../test/t10k-labels.idx1-ubyte\"\n",
    "\n",
    "interview_data = idx2numpy.convert_from_file(interview_data_path)\n",
    "interview_data = np.expand_dims(interview_data, axis=1)\n",
    "interview_data = torch.from_numpy(interview_data).float().to(device)\n",
    "\n",
    "interview_label = idx2numpy.convert_from_file(interview_label_path)\n",
    "interview_labels = one_hot(interview_label, 10)\n",
    "interview_labels = torch.from_numpy(interview_labels).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d9bda69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interview Accuracy: 99.54%\n"
     ]
    }
   ],
   "source": [
    "# For interview\n",
    "model.load_state_dict(torch.load(modelpath))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred = model(interview_data)\n",
    "    pred_labels = torch.argmax(pred, dim=1)\n",
    "    true_labels = torch.argmax(interview_labels, dim=1)\n",
    "    accuracy = torch.sum(pred_labels == true_labels).item() / len(interview_data)\n",
    "    print(\"Interview Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
