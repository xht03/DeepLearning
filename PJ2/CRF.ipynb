{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "062be403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from check import check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b62e58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF:\n",
    "    def __init__(self, tags, feature_templates):\n",
    "        self.tags = tags\n",
    "        self.tag2idx = {tag: idx for idx, tag in enumerate(tags)}\n",
    "        self.feature_templates = feature_templates\n",
    "        self.weights = defaultdict(float)                               # 特征权重\n",
    "        self.transition = np.random.randn(len(tags), len(tags)) * 0.01  # 转移矩阵\n",
    "\n",
    "    # 提取特征\n",
    "    def extract_features(self, seq, pos, prev_tag, current_tag):\n",
    "        features = []\n",
    "        for template in self.feature_templates:\n",
    "            if \":\" not in template:\n",
    "                print(\"模板格式错误：\", template)\n",
    "                continue\n",
    "            # Unigram特征\n",
    "            if template.startswith(\"U\"):\n",
    "                parts = template.split(\":\")[1].split(\"/\")\n",
    "                context = []\n",
    "                for part in parts:\n",
    "                    off = int(part[3:-1].split(\",\")[0])\n",
    "                    idx = pos + off\n",
    "                    if idx < 0:\n",
    "                        context.append(\"[BEG]\")\n",
    "                    elif idx >= len(seq):\n",
    "                        context.append(\"[END]\")\n",
    "                    else:\n",
    "                        context.append(seq[idx])\n",
    "                features.append(f\"{current_tag}::{template}:{'/'.join(context)}\")\n",
    "            # Bigram特征\n",
    "            elif template.startswith(\"B\"):\n",
    "                if prev_tag is not None:\n",
    "                    parts = template.split(\":\")[1].split(\"/\")\n",
    "                    context = []\n",
    "                    for part in parts:\n",
    "                        off = int(part[3:-1].split(\",\")[0])\n",
    "                        idx = pos + off\n",
    "                        if idx < 0:\n",
    "                            context.append(\"[BEG]\")\n",
    "                        elif idx >= len(seq):\n",
    "                            context.append(\"[END]\")\n",
    "                        else:\n",
    "                            context.append(seq[idx])\n",
    "                    features.append(f\"{prev_tag}->{current_tag}::{template}:{'/'.join(context)}\")\n",
    "        return features\n",
    "                \n",
    "    # 前向算法\n",
    "    # alpha[t, tag]: 给定观测序列的前t+1个词时，以tag作为第t个词的标签的所有路径的“分数”之和\n",
    "    # Z: 配分函数（所有可能路径的得分和）\n",
    "    def forward(self, seq):\n",
    "        T = len(seq)\n",
    "        N = len(self.tags)\n",
    "        alpha = np.zeros((T, N))\n",
    "\n",
    "        # 初始化\n",
    "        for i in range(N):\n",
    "            features = self.extract_features(seq, 0, None, self.tags[i])\n",
    "            alpha[0][i] = sum(self.weights[f] for f in features)\n",
    "        \n",
    "        # 递推\n",
    "        for t in range(1, T):\n",
    "            for tag in range(N):\n",
    "                score = 0\n",
    "                for prev_tag in range(N):\n",
    "                    features = self.extract_features(seq, t, self.tags[prev_tag], self.tags[tag])\n",
    "                    trans_score = self.transition[prev_tag, tag]\n",
    "                    emit_score = sum(self.weights[f] for f in features)\n",
    "                    score += np.exp(alpha[t - 1][prev_tag] + trans_score + emit_score)\n",
    "                alpha[t, tag] = np.log(score) if score > 0 else -np.inf\n",
    "\n",
    "        log_Z = np.log(sum(np.exp(alpha[-1]))) if any(np.isfinite(alpha[-1])) else -np.inf\n",
    "        return alpha, log_Z\n",
    "\n",
    "    # 后向算法\n",
    "    # beta[t, tag]: 在给定观测序列的第t个词处，已知该词的标签为tag，从t到序列末尾的所有可能标签路径的“分数”之和。\n",
    "    def backward(self, seq):\n",
    "        T = len(seq)\n",
    "        N = len(self.tags)\n",
    "        beta = np.zeros((T, N))\n",
    "        \n",
    "        # 初始化\n",
    "        beta[T-1, :] = 0\n",
    "\n",
    "        # 递推\n",
    "        for t in range(T-2, -1, -1):\n",
    "            for tag in range(N):\n",
    "                score = 0\n",
    "                for next_tag in range(N):\n",
    "                    features = self.extract_features(seq, t+1, self.tags[tag], self.tags[next_tag])\n",
    "                    trans_score = self.transition[tag, next_tag]\n",
    "                    emit_score = sum(self.weights[f] for f in features)\n",
    "                    score += np.exp(beta[t+1, next_tag] + trans_score + emit_score)\n",
    "                beta[t, tag] = np.log(score) if score > 0 else -np.inf\n",
    "        return beta\n",
    "    \n",
    "    # 维特比解码\n",
    "    def viterbi_decode(self, seq):\n",
    "        T = len(seq)\n",
    "        N = len(self.tags)\n",
    "        viterbi = np.zeros((T, N))\n",
    "        backptrs = np.zeros((T, N), dtype=int)    # 回溯指针\n",
    "\n",
    "        # 初始化\n",
    "        for tag in range(N):\n",
    "            features = self.extract_features(seq, 0, None, self.tags[tag])\n",
    "            viterbi[0, tag] = sum(self.weights[f] for f in features)\n",
    "\n",
    "        # 递推\n",
    "        for t in range(1, T):\n",
    "            for i in range(N):        # 当前标签\n",
    "                max_score = -np.inf\n",
    "                best_prev_tag = 0\n",
    "                for j in range(N):     # 前一个标签\n",
    "                    trans_score = self.transition[j][i]\n",
    "                    features = self.extract_features(seq, t, self.tags[j], self.tags[i])\n",
    "                    emit_score = sum(self.weights[f] for f in features)\n",
    "                    score = viterbi[t-1][j] + trans_score + emit_score\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "                        best_prev_tag = j\n",
    "                viterbi[t][i] = max_score\n",
    "                backptrs[t][i] = best_prev_tag\n",
    "\n",
    "        # 回溯\n",
    "        best_path = [np.argmax(viterbi[-1])]\n",
    "        for t in reversed(range(1, T)):\n",
    "            best_path.append(backptrs[t][best_path[-1]])\n",
    "        best_path.reverse()\n",
    "\n",
    "        return [self.tags[i] for i in best_path]\n",
    "    \n",
    "    def compute_gradients(self, seq, tag_seq):\n",
    "        T = len(seq)\n",
    "        N = len(self.tags)\n",
    "\n",
    "        # 经验特征（真实情况中出现过的特征）\n",
    "        empirical_features = set()\n",
    "        for t in range(T):\n",
    "            prev_tag = tag_seq[t-1] if t > 0 else None\n",
    "            features = self.extract_features(seq, t, prev_tag, tag_seq[t])\n",
    "            empirical_features.update(features)\n",
    "\n",
    "        # 期望特征（模型算出来的特征）\n",
    "        alpha, log_Z = self.forward(seq)\n",
    "        beta = self.backward(seq)\n",
    "        expected_features = defaultdict(float)\n",
    "\n",
    "        # t=0\n",
    "        for i in range(N):\n",
    "            features = self.extract_features(seq, 0, None, self.tags[i])\n",
    "            p = np.exp(alpha[0][i] + beta[0][i] - log_Z) if log_Z != -np.inf else 0\n",
    "            for f in features:\n",
    "                expected_features[f] += p\n",
    "\n",
    "        # t>=1\n",
    "        for t in range(1, T):\n",
    "            for i in range(N):      # current tag\n",
    "                for j in range(N):  # prev tag\n",
    "                    features = self.extract_features(seq, t, self.tags[j], self.tags[i])\n",
    "                    trans_score = self.transition[i][j]\n",
    "                    emit_score = sum(self.weights[f] for f in features)\n",
    "                    p = np.exp(alpha[t-1][i] + trans_score + emit_score + beta[t][j] - log_Z) if log_Z != -np.inf else 0\n",
    "                    for f in features:\n",
    "                        expected_features[f] += p\n",
    "        \n",
    "        # 计算 weights 梯度\n",
    "        w_gradient = defaultdict(float)\n",
    "        for f in empirical_features:\n",
    "            w_gradient[f] += 1  # 真实特征计数\n",
    "        for f in expected_features:\n",
    "            w_gradient[f] -= expected_features[f]\n",
    "\n",
    "        # 计算 transition 矩阵梯度\n",
    "        t_gradient = np.zeros((N, N))\n",
    "        for t in range(1, T):\n",
    "            # empirical\n",
    "            i = self.tag2idx[tag_seq[t-1]]\n",
    "            j = self.tag2idx[tag_seq[t]]\n",
    "            t_gradient[i][j] += 1\n",
    "\n",
    "            # expected\n",
    "            for i_ in range(N):\n",
    "                for j_ in range(N):\n",
    "                    features = self.extract_features(seq, t, self.tags[i_], self.tags[j_])\n",
    "                    p = np.exp(alpha[t-1][i_] + self.transition[i_][j_] + sum(self.weights[f] for f in features) + beta[t][j_] - log_Z) if log_Z != -np.inf else 0\n",
    "                    t_gradient[i_][j_] -= p\n",
    "\n",
    "        return w_gradient, t_gradient, log_Z\n",
    "\n",
    "\n",
    "    # 训练CRF模型\n",
    "    def train(self, seqs, tag_seqs, lr=0.1, max_iter=10):\n",
    "        for iter in range(max_iter):\n",
    "            avg_loss = 0\n",
    "            for seq, tag_seq in zip(seqs, tag_seqs):\n",
    "                w_grad, t_grad, log_Z = self.compute_gradients(seq, tag_seq)\n",
    "\n",
    "                for f in w_grad:\n",
    "                    self.weights[f] += lr * w_grad[f]\n",
    "                \n",
    "                self.transition += lr * t_grad\n",
    "                \n",
    "                # 计算损失\n",
    "                score = 0\n",
    "                for t in range(len(seq)):\n",
    "                    prev_tag = tag_seq[t-1] if t > 0 else None\n",
    "                    features = self.extract_features(seq, t, prev_tag, tag_seq[t])\n",
    "                    score += sum(self.weights[f] for f in features)\n",
    "                    if t > 0:\n",
    "                        i = self.tag2idx[tag_seq[t-1]]\n",
    "                        j = self.tag2idx[tag_seq[t]]\n",
    "                        score += self.transition[i, j]\n",
    "                avg_loss += log_Z - score\n",
    "\n",
    "            avg_loss /= len(seqs)\n",
    "            print(f\"Iteration {iter}, Loss: {avg_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "8a02b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_templates = [\n",
    "    \"U00:%x[-2,0]\",\n",
    "    \"U01:%x[-1,0]\",\n",
    "    \"U02:%x[0,0]\",\n",
    "    \"U03:%x[1,0]\",\n",
    "    \"U04:%x[2,0]\",\n",
    "    \"U05:%x[-2,0]/%x[-1,0]\",\n",
    "    \"U06:%x[-1,0]/%x[0,0]\",\n",
    "    \"U07:%x[-1,0]/%x[1,0]\",\n",
    "    \"U08:%x[0,0]/%x[1,0]\",\n",
    "    \"U09:%x[1,0]/%x[2,0]\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9c6ffe5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 4.843256297009441\n",
      "Iteration 1, Loss: 1.162976847863514\n",
      "Iteration 2, Loss: -1.0869853298223768\n",
      "Iteration 3, Loss: -2.5001258395600354\n",
      "Iteration 4, Loss: -3.2525668685552915\n",
      "Iteration 5, Loss: -3.5916067878546905\n",
      "Iteration 6, Loss: -3.7321245985288627\n",
      "Iteration 7, Loss: -3.7889379629642352\n",
      "Iteration 8, Loss: -3.811516540115367\n",
      "Iteration 9, Loss: -3.8197554907994764\n",
      "Iteration 10, Loss: -3.821629253774411\n",
      "Iteration 11, Loss: -3.8204288592528926\n",
      "Iteration 12, Loss: -3.8175880221557357\n",
      "Iteration 13, Loss: -3.8137842825752024\n",
      "Iteration 14, Loss: -3.8093746650367706\n",
      "Iteration 15, Loss: -3.804572129219892\n",
      "Iteration 16, Loss: -3.799519182463861\n",
      "Iteration 17, Loss: -3.7943197592669122\n",
      "Iteration 18, Loss: -3.7890537305995906\n",
      "Iteration 19, Loss: -3.7837839854542494\n",
      "Iteration 20, Loss: -3.7785602487129566\n",
      "Iteration 21, Loss: -3.7734214270216384\n",
      "Iteration 22, Loss: -3.768397271996932\n",
      "Iteration 23, Loss: -3.763509712582234\n",
      "Iteration 24, Loss: -3.7587740111429824\n",
      "Iteration 25, Loss: -3.754199806991842\n",
      "Iteration 26, Loss: -3.7497920693567792\n",
      "Iteration 27, Loss: -3.7455519639014696\n",
      "Iteration 28, Loss: -3.7414776307444058\n",
      "Iteration 29, Loss: -3.73756487144572\n",
      "Iteration 30, Loss: -3.733807744432589\n",
      "Iteration 31, Loss: -3.730199071080463\n",
      "Iteration 32, Loss: -3.7267308572537132\n",
      "Iteration 33, Loss: -3.7233946371190996\n",
      "Iteration 34, Loss: -3.720181747346828\n",
      "Iteration 35, Loss: -3.717083540440555\n",
      "Iteration 36, Loss: -3.714091545997775\n",
      "Iteration 37, Loss: -3.7111975883381234\n",
      "Iteration 38, Loss: -3.7083938682844177\n",
      "Iteration 39, Loss: -3.7056730160642246\n",
      "Iteration 40, Loss: -3.703028121404728\n",
      "Iteration 41, Loss: -3.700452746000977\n",
      "Iteration 42, Loss: -3.697940922688346\n",
      "Iteration 43, Loss: -3.6954871448727147\n",
      "Iteration 44, Loss: -3.6930863490875367\n",
      "Iteration 45, Loss: -3.690733892952352\n",
      "Iteration 46, Loss: -3.6884255303067874\n",
      "Iteration 47, Loss: -3.6861573848772764\n",
      "Iteration 48, Loss: -3.683925923493092\n",
      "Iteration 49, Loss: -3.681727929594885\n",
      "预测结果: [('王', 'I-NAME'), ('五', 'I-NAME'), ('来自', 'O'), ('上海', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# 定义标签集\n",
    "tags = [\"O\", \"B-NAME\", \"I-NAME\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]\n",
    "\n",
    "# 初始化CRF\n",
    "crf = CRF(tags, feature_templates)\n",
    "\n",
    "# 训练数据示例\n",
    "train_sentences = [[\"张\", \"三\", \"在\", \"北京\", \"工作\"], [\"李\", \"四\", \"是\", \"腾讯\", \"员工\"]]\n",
    "train_tags = [[\"B-NAME\", \"I-NAME\", \"O\", \"B-LOC\", \"O\"], [\"B-NAME\", \"I-NAME\", \"O\", \"B-ORG\", \"O\"]]\n",
    "\n",
    "# 训练模型\n",
    "crf.train(train_sentences, train_tags, max_iter=50)\n",
    "\n",
    "# 预测新句子\n",
    "test_sentence = [\"王\", \"五\", \"来自\", \"上海\"]\n",
    "predicted_tags = crf.viterbi_decode(test_sentence)\n",
    "print(\"预测结果:\", list(zip(test_sentence, predicted_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "802cb738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_path):\n",
    "    # 初始化数据结构\n",
    "    tags = set()\n",
    "    words = set()\n",
    "    tag_seqs = []\n",
    "    word_seqs = []\n",
    "    \n",
    "    current_state_seq = []\n",
    "    current_obs_seq = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:  # 空行表示句子结束\n",
    "                if current_state_seq and current_obs_seq:\n",
    "                    tag_seqs.append(current_state_seq)\n",
    "                    word_seqs.append(current_obs_seq)\n",
    "                    current_state_seq = []\n",
    "                    current_obs_seq = []\n",
    "                continue\n",
    "                \n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:  # 确保有词和标签\n",
    "                word = parts[0]\n",
    "                tag = parts[-1]  # 假设标签在最后\n",
    "                \n",
    "                # 更新状态和观测集合\n",
    "                tags.add(tag)\n",
    "                words.add(word)\n",
    "                \n",
    "                # 添加到当前序列\n",
    "                current_state_seq.append(tag)\n",
    "                current_obs_seq.append(word)\n",
    "    \n",
    "    # 处理最后一个句子（如果文件不以空行结尾）\n",
    "    if current_state_seq and current_obs_seq:\n",
    "        tag_seqs.append(current_state_seq)\n",
    "        word_seqs.append(current_obs_seq)\n",
    "    \n",
    "    # 转换为列表并排序（为了确定性）\n",
    "    tags = sorted(tags)\n",
    "    words = sorted(words)\n",
    "    \n",
    "    return {\n",
    "        'tags': tags,\n",
    "        'words': words,\n",
    "        'tag_seqs': tag_seqs,\n",
    "        'word_seqs': word_seqs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ebefba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练数据\n",
    "train_data_path = \"./NER/Chinese/mytrain.txt\"\n",
    "train_data = process_data(train_data_path)\n",
    "\n",
    "tags = train_data['tags']\n",
    "words = train_data['words']\n",
    "tag_seqs = train_data['tag_seqs']\n",
    "word_seqs = train_data['word_seqs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "fcc02cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 21.494153608466444\n",
      "Iteration 1, Loss: 4.447593915613142\n",
      "Iteration 2, Loss: -0.11345884484030933\n",
      "Iteration 3, Loss: -2.7877959686463543\n",
      "Iteration 4, Loss: -4.235524016701729\n",
      "Iteration 5, Loss: -4.958916460977056\n",
      "Iteration 6, Loss: -5.326304825290416\n",
      "Iteration 7, Loss: -5.533958328757754\n",
      "Iteration 8, Loss: -5.670547631783288\n",
      "Iteration 9, Loss: -5.770158177852323\n",
      "Iteration 10, Loss: -5.847598767109531\n",
      "Iteration 11, Loss: -5.9108032322493305\n",
      "Iteration 12, Loss: -5.964327642571564\n",
      "Iteration 13, Loss: -6.010911011760049\n",
      "Iteration 14, Loss: -6.052302272184852\n",
      "Iteration 15, Loss: -6.089690704707027\n",
      "Iteration 16, Loss: -6.123947397093207\n",
      "Iteration 17, Loss: -6.155782444065613\n",
      "Iteration 18, Loss: -6.18586259288104\n",
      "Iteration 19, Loss: -6.214900141041698\n",
      "Iteration 20, Loss: -6.243690862156052\n",
      "Iteration 21, Loss: -6.273044819068491\n",
      "Iteration 22, Loss: -6.303570931209265\n",
      "Iteration 23, Loss: -6.335407527190602\n",
      "Iteration 24, Loss: -6.368129108352713\n",
      "Iteration 25, Loss: -6.400950905459825\n",
      "Iteration 26, Loss: -6.4330686338647185\n",
      "Iteration 27, Loss: -6.463892353201277\n",
      "Iteration 28, Loss: -6.493101941401273\n",
      "Iteration 29, Loss: -6.5205956133119685\n",
      "Iteration 30, Loss: -6.546414570802125\n",
      "Iteration 31, Loss: -6.570681809597931\n",
      "Iteration 32, Loss: -6.593559259136612\n",
      "Iteration 33, Loss: -6.615216947352986\n",
      "Iteration 34, Loss: -6.6358100091422445\n",
      "Iteration 35, Loss: -6.655463573717919\n",
      "Iteration 36, Loss: -6.674267021233038\n",
      "Iteration 37, Loss: inf\n",
      "Iteration 38, Loss: nan\n",
      "Iteration 39, Loss: nan\n",
      "Iteration 40, Loss: nan\n",
      "Iteration 41, Loss: nan\n",
      "Iteration 42, Loss: nan\n",
      "Iteration 43, Loss: nan\n",
      "Iteration 44, Loss: nan\n",
      "Iteration 45, Loss: nan\n",
      "Iteration 46, Loss: nan\n",
      "Iteration 47, Loss: nan\n",
      "Iteration 48, Loss: nan\n",
      "Iteration 49, Loss: nan\n",
      "Iteration 50, Loss: nan\n",
      "Iteration 51, Loss: nan\n",
      "Iteration 52, Loss: nan\n",
      "Iteration 53, Loss: nan\n",
      "Iteration 54, Loss: nan\n",
      "Iteration 55, Loss: nan\n",
      "Iteration 56, Loss: nan\n",
      "Iteration 57, Loss: nan\n",
      "Iteration 58, Loss: nan\n",
      "Iteration 59, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# 训练CRF模型\n",
    "crf = CRF(tags, feature_templates)\n",
    "crf.train(word_seqs, tag_seqs, max_iter=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "68c692f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_data_path = \"./NER/Chinese/validation.txt\"\n",
    "valid_data_path = \"./NER/Chinese/myvalid.txt\"\n",
    "valid_data = process_data(valid_data_path)\n",
    "\n",
    "output_path = \"./crf_validation_output.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for seq, tag_seq in zip(valid_data['word_seqs'], valid_data['tag_seqs']):\n",
    "        predicted_tags = crf.viterbi_decode(seq)\n",
    "        for word, tag in zip(seq, predicted_tags):\n",
    "            fout.write(f\"{word} {tag}\\n\")\n",
    "        fout.write(\"\\n\")  # 句子间空行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9361f677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-PER     0.0000    0.0000    0.0000         0\n",
      "       I-PER     0.0000    0.0000    0.0000         0\n",
      "       B-ORG     0.0000    0.0000    0.0000         2\n",
      "       I-ORG     0.0000    0.0000    0.0000         0\n",
      "       B-LOC     0.0000    0.0000    0.0000         0\n",
      "       I-LOC     0.0000    0.0000    0.0000         0\n",
      "      B-MISC     0.0000    0.0000    0.0000         0\n",
      "      I-MISC     0.0000    0.0000    0.0000         0\n",
      "\n",
      "   micro avg     0.0000    0.0000    0.0000         2\n",
      "   macro avg     0.0000    0.0000    0.0000         2\n",
      "weighted avg     0.0000    0.0000    0.0000         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check(language = \"English\", gold_path=\"NER/Chinese/myvalid.txt\", my_path=\"crf_validation_output.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
