{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "062be403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from check import check\n",
    "import pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b62e58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRF:\n",
    "    def __init__(self, tags, feature_templates):\n",
    "        self.tags = tags\n",
    "        self.tag2idx = {tag: idx for idx, tag in enumerate(tags)}\n",
    "        self.feature_templates = feature_templates\n",
    "        self.weights = defaultdict(float)                               # 特征权重\n",
    "        self.transition = np.random.randn(len(tags), len(tags)) * 0.01  # 转移矩阵\n",
    "\n",
    "    def save(self, filepath):\n",
    "        # 保存权重和转移矩阵\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"weights\": dict(self.weights),\n",
    "                \"transition\": self.transition,\n",
    "                \"tags\": self.tags,\n",
    "                \"feature_templates\": self.feature_templates\n",
    "            }, f)\n",
    "\n",
    "    def load(self, filepath):\n",
    "        # 加载权重和转移矩阵\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            self.weights = defaultdict(float, data[\"weights\"])\n",
    "            self.transition = data[\"transition\"]\n",
    "            self.tags = data[\"tags\"]\n",
    "            self.tag2idx = {tag: idx for idx, tag in enumerate(self.tags)}\n",
    "            self.feature_templates = data[\"feature_templates\"]\n",
    "\n",
    "    # 提取特征\n",
    "    def extract_features(self, seq, pos, prev_tag, current_tag):\n",
    "        features = []\n",
    "        for template in self.feature_templates:\n",
    "            if \":\" not in template:\n",
    "                print(\"模板格式错误：\", template)\n",
    "                continue\n",
    "            # Unigram特征\n",
    "            if template.startswith(\"U\"):\n",
    "                parts = template.split(\":\")[1].split(\"/\")\n",
    "                context = []\n",
    "                for part in parts:\n",
    "                    off = int(part[3:-1].split(\",\")[0])\n",
    "                    idx = pos + off\n",
    "                    if idx < 0:\n",
    "                        context.append(\"[BEG]\")\n",
    "                    elif idx >= len(seq):\n",
    "                        context.append(\"[END]\")\n",
    "                    else:\n",
    "                        context.append(seq[idx])\n",
    "                features.append(f\"{current_tag}::{template}:{'/'.join(context)}\")\n",
    "            # Bigram特征\n",
    "            elif template.startswith(\"B\"):\n",
    "                if prev_tag is not None:\n",
    "                    parts = template.split(\":\")[1].split(\"/\")\n",
    "                    context = []\n",
    "                    for part in parts:\n",
    "                        off = int(part[3:-1].split(\",\")[0])\n",
    "                        idx = pos + off\n",
    "                        if idx < 0:\n",
    "                            context.append(\"[BEG]\")\n",
    "                        elif idx >= len(seq):\n",
    "                            context.append(\"[END]\")\n",
    "                        else:\n",
    "                            context.append(seq[idx])\n",
    "                    features.append(f\"{prev_tag}->{current_tag}::{template}:{'/'.join(context)}\")\n",
    "        return features\n",
    "                \n",
    "    # 前向算法\n",
    "    # alpha[t, tag]: 给定观测序列的前t+1个词时，以tag作为第t个词的标签的所有路径的“分数”之和\n",
    "    # Z: 配分函数（所有可能路径的得分和）\n",
    "    def forward(self, seq):\n",
    "        T = len(seq)\n",
    "        N = len(self.tags)\n",
    "        alpha = np.zeros((T, N))\n",
    "\n",
    "        # 初始化\n",
    "        for i in range(N):\n",
    "            features = self.extract_features(seq, 0, None, self.tags[i])\n",
    "            alpha[0][i] = sum(self.weights[f] for f in features)\n",
    "        \n",
    "        # 递推\n",
    "        for t in range(1, T):\n",
    "            for tag in range(N):\n",
    "                log_scores = []\n",
    "                for prev_tag in range(N):\n",
    "                    features = self.extract_features(seq, t, self.tags[prev_tag], self.tags[tag])\n",
    "                    trans_score = self.transition[prev_tag, tag]\n",
    "                    emit_score = sum(self.weights[f] for f in features)\n",
    "                    log_scores.append(alpha[t - 1][prev_tag] + trans_score + emit_score)\n",
    "                alpha[t, tag] = np.logaddexp.reduce(log_scores) if log_scores else -np.inf\n",
    "\n",
    "        log_Z = np.log(sum(np.exp(alpha[-1]))) if any(np.isfinite(alpha[-1])) else -np.inf\n",
    "\n",
    "        return alpha, log_Z\n",
    "\n",
    "    # 后向算法\n",
    "    # beta[t, tag]: 在给定观测序列的第t个词处，已知该词的标签为tag，从t到序列末尾的所有可能标签路径的“分数”之和。\n",
    "    def backward(self, seq):\n",
    "        T = len(seq)\n",
    "        N = len(self.tags)\n",
    "        beta = np.zeros((T, N))\n",
    "        \n",
    "        # 初始化\n",
    "        beta[T-1, :] = 0\n",
    "\n",
    "        # 递推\n",
    "        for t in range(T-2, -1, -1):\n",
    "            for tag in range(N):\n",
    "                log_scores = []\n",
    "                for next_tag in range(N):\n",
    "                    features = self.extract_features(seq, t+1, self.tags[tag], self.tags[next_tag])\n",
    "                    trans_score = self.transition[tag, next_tag]\n",
    "                    emit_score = sum(self.weights[f] for f in features)\n",
    "                    log_scores.append(beta[t+1, next_tag] + trans_score + emit_score)\n",
    "                beta[t, tag] = np.logaddexp.reduce(log_scores) if log_scores else -np.inf\n",
    "        return beta\n",
    "    \n",
    "    # 维特比解码\n",
    "    def viterbi_decode(self, seq):\n",
    "        T = len(seq)\n",
    "        N = len(self.tags)\n",
    "        viterbi = np.zeros((T, N))\n",
    "        backptrs = np.zeros((T, N), dtype=int)    # 回溯指针\n",
    "\n",
    "        # 初始化\n",
    "        for tag in range(N):\n",
    "            features = self.extract_features(seq, 0, None, self.tags[tag])\n",
    "            viterbi[0, tag] = sum(self.weights[f] for f in features)\n",
    "\n",
    "        # 递推\n",
    "        for t in range(1, T):\n",
    "            for i in range(N):        # 当前标签\n",
    "                max_score = -np.inf\n",
    "                best_prev_tag = 0\n",
    "                for j in range(N):     # 前一个标签\n",
    "                    trans_score = self.transition[j][i]\n",
    "                    features = self.extract_features(seq, t, self.tags[j], self.tags[i])\n",
    "                    emit_score = sum(self.weights[f] for f in features)\n",
    "                    score = viterbi[t-1][j] + trans_score + emit_score\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "                        best_prev_tag = j\n",
    "                viterbi[t][i] = max_score\n",
    "                backptrs[t][i] = best_prev_tag\n",
    "\n",
    "        # 回溯\n",
    "        best_path = [np.argmax(viterbi[-1])]\n",
    "        for t in reversed(range(1, T)):\n",
    "            best_path.append(backptrs[t][best_path[-1]])\n",
    "        best_path.reverse()\n",
    "\n",
    "        return [self.tags[i] for i in best_path]\n",
    "    \n",
    "    # 计算梯度\n",
    "    def compute_gradients(self, seq, tag_seq):\n",
    "        T = len(seq)\n",
    "        N = len(self.tags)\n",
    "\n",
    "        # 真实路径特征\n",
    "        true_features = set()\n",
    "        for t in range(T):\n",
    "            prev_tag = tag_seq[t - 1] if t > 0 else None\n",
    "            features = self.extract_features(seq, t, prev_tag, tag_seq[t])\n",
    "            true_features.update(features)\n",
    "\n",
    "        # 前向后向算法\n",
    "        alpha, log_Z = self.forward(seq)\n",
    "        beta = self.backward(seq)\n",
    "        expected_features = defaultdict(float)\n",
    "\n",
    "        # 计算特征期望\n",
    "        for t in range(T):\n",
    "            for i in range(N):\n",
    "                for j in range(N):\n",
    "                    # 提取特征\n",
    "                    cur_tag = self.tags[j]\n",
    "                    prev_tag = self.tags[i] if t > 0 else None\n",
    "                    features = self.extract_features(seq, t, prev_tag, cur_tag)\n",
    "\n",
    "                    # 计算概率\n",
    "                    if t == 0:\n",
    "                        prob = np.exp(alpha[t][j] + beta[t][j] - log_Z) if log_Z != -np.inf else 0\n",
    "                    else:\n",
    "                        trans_score = self.transition[i][j]\n",
    "                        emit_score = sum(self.weights[f] for f in features)\n",
    "                        prob = np.exp(alpha[t - 1][i] + trans_score + emit_score + beta[t][j] - log_Z) if log_Z != -np.inf else 0\n",
    "\n",
    "                    # 累加特征期望\n",
    "                    for f in features:\n",
    "                        expected_features[f] += prob\n",
    "\n",
    "        # 计算权重梯度\n",
    "        # weight_grad[f] = true - expected\n",
    "        w_grad = defaultdict(float)\n",
    "        for f in true_features:\n",
    "            w_grad[f] += 1\n",
    "        for f in expected_features:\n",
    "            w_grad[f] -= expected_features[f]\n",
    "\n",
    "        # 计算转移矩阵梯度\n",
    "        # transition_grad[f] = true - expected\n",
    "        t_grad = np.zeros_like(self.transition)\n",
    "        for t in range(1, T):\n",
    "            i = self.tag2idx[tag_seq[t - 1]]\n",
    "            j = self.tag2idx[tag_seq[t]]\n",
    "            t_grad[i][j] += 1\n",
    "\n",
    "            for i_ in range(N):\n",
    "                for j_ in range(N):\n",
    "                    features = self.extract_features(seq, t, self.tags[i_], self.tags[j_])\n",
    "                    prob = np.exp(alpha[t - 1][i_] + self.transition[i_][j_] + sum(self.weights[f] for f in features) + beta[t][j_] - log_Z) if log_Z != -np.inf else 0\n",
    "                    t_grad[i_][j_] -= prob\n",
    "\n",
    "        return w_grad, t_grad, log_Z\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # 训练CRF模型\n",
    "    def train(self, seqs, tag_seqs, batch_size, max_iter, learning_rate):\n",
    "        for iter in range(max_iter):\n",
    "            total_loss = 0\n",
    "            batch_indices = range(0, len(seqs), batch_size)\n",
    "\n",
    "            for start_idx in batch_indices:\n",
    "                end_idx = start_idx + batch_size\n",
    "                batch_sentences = seqs[start_idx:end_idx]\n",
    "                batch_tags = tag_seqs[start_idx:end_idx]\n",
    "                print(f\"正在处理批次 {start_idx}-{end_idx}/{len(seqs)} ...\")\n",
    "\n",
    "                # 初始化\n",
    "                batch_weights_grad = defaultdict(float)\n",
    "                batch_transition_grad = np.zeros_like(self.transition)\n",
    "                batch_loss = 0.0\n",
    "\n",
    "                # 计算批次内所有样本的梯度\n",
    "                for sentence, tags in zip(batch_sentences, batch_tags):\n",
    "                    # 计算单个样本的梯度\n",
    "                    weights_grad, transition_grad, log_Z = self.compute_gradients(sentence, tags)\n",
    "\n",
    "                    # 累积权重梯度\n",
    "                    for f in weights_grad:\n",
    "                        batch_weights_grad[f] += weights_grad[f]\n",
    "\n",
    "                    # 累积转移矩阵梯度\n",
    "                    batch_transition_grad += transition_grad\n",
    "\n",
    "                    # 计算单个样本的损失\n",
    "                    true_score = self.compute_single_score(sentence, tags)\n",
    "                    batch_loss += log_Z - true_score\n",
    "\n",
    "                # 计算批次平均梯度\n",
    "                batch_size_actual = len(batch_sentences)\n",
    "                for f in batch_weights_grad:\n",
    "                    batch_weights_grad[f] /= batch_size_actual\n",
    "                batch_transition_grad /= batch_size_actual\n",
    "                batch_loss /= batch_size_actual\n",
    "\n",
    "                # 使用平均梯度\n",
    "                for f in batch_weights_grad:\n",
    "                    self.weights[f] += learning_rate * batch_weights_grad[f]\n",
    "\n",
    "                for i in range(len(self.tags)):\n",
    "                    for j in range(len(self.tags)):\n",
    "                        self.transition[i][j] += learning_rate * batch_transition_grad[i][j]\n",
    "\n",
    "                loss = batch_loss * batch_size_actual\n",
    "                print(f\"批次损失: {loss/batch_size_actual:.2f}\")\n",
    "                total_loss += loss\n",
    "                \n",
    "            print(f\"第 {iter} 次迭代, Loss={total_loss/len(seqs):.2f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    # 计算单个样本的经验路径得分\n",
    "    def compute_single_score(self, seq, tag_seq):\n",
    "        score = 0\n",
    "        for t in range(len(seq)):\n",
    "            prev_tag = tag_seq[t - 1] if t > 0 else None\n",
    "            features = self.extract_features(seq, t, prev_tag, tag_seq[t])\n",
    "            score += sum(self.weights[f] for f in features)\n",
    "            if t > 0:\n",
    "                i = self.tag2idx[tag_seq[t - 1]]\n",
    "                j = self.tag2idx[tag_seq[t]]\n",
    "                score += self.transition[i][j]\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a02b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_templates = [\n",
    "    \"U00:%x[-2,0]\",\n",
    "    \"U01:%x[-1,0]\",\n",
    "    \"U02:%x[0,0]\",\n",
    "    \"U03:%x[1,0]\",\n",
    "    \"U04:%x[2,0]\",\n",
    "    \"U05:%x[-2,0]/%x[-1,0]\",\n",
    "    \"U06:%x[-1,0]/%x[0,0]\",\n",
    "    \"U07:%x[-1,0]/%x[1,0]\",\n",
    "    \"U08:%x[0,0]/%x[1,0]\",\n",
    "    \"U09:%x[1,0]/%x[2,0]\",\n",
    "    \"B00:%x[-2,0]\",\n",
    "    \"B01:%x[-1,0]\",\n",
    "    \"B02:%x[0,0]\",\n",
    "    \"B03:%x[1,0]\",\n",
    "    \"B04:%x[2,0]\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "802cb738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_path):\n",
    "    # 初始化数据结构\n",
    "    tags = set()\n",
    "    words = set()\n",
    "    tag_seqs = []\n",
    "    word_seqs = []\n",
    "    \n",
    "    current_state_seq = []\n",
    "    current_obs_seq = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:  # 空行表示句子结束\n",
    "                if current_state_seq and current_obs_seq:\n",
    "                    tag_seqs.append(current_state_seq)\n",
    "                    word_seqs.append(current_obs_seq)\n",
    "                    current_state_seq = []\n",
    "                    current_obs_seq = []\n",
    "                continue\n",
    "                \n",
    "            parts = line.split()\n",
    "            if len(parts) >= 2:  # 确保有词和标签\n",
    "                word = parts[0]\n",
    "                tag = parts[-1]  # 假设标签在最后\n",
    "                \n",
    "                # 更新状态和观测集合\n",
    "                tags.add(tag)\n",
    "                words.add(word)\n",
    "                \n",
    "                # 添加到当前序列\n",
    "                current_state_seq.append(tag)\n",
    "                current_obs_seq.append(word)\n",
    "    \n",
    "    # 处理最后一个句子（如果文件不以空行结尾）\n",
    "    if current_state_seq and current_obs_seq:\n",
    "        tag_seqs.append(current_state_seq)\n",
    "        word_seqs.append(current_obs_seq)\n",
    "    \n",
    "    # 转换为列表并排序（为了确定性）\n",
    "    tags = sorted(tags)\n",
    "    words = sorted(words)\n",
    "    \n",
    "    return {\n",
    "        'tags': tags,\n",
    "        'words': words,\n",
    "        'tag_seqs': tag_seqs,\n",
    "        'word_seqs': word_seqs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebefba3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 加载训练数据\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# train_data_path = \"./NER/Chinese/mytrain.txt\"\u001b[39;00m\n\u001b[32m      3\u001b[39m train_data_path = \u001b[33m\"\u001b[39m\u001b[33m./NER/English/mytrain.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m train_data = \u001b[43mprocess_data\u001b[49m(train_data_path)\n\u001b[32m      6\u001b[39m tags = train_data[\u001b[33m'\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m words = train_data[\u001b[33m'\u001b[39m\u001b[33mwords\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'process_data' is not defined"
     ]
    }
   ],
   "source": [
    "# 加载训练数据\n",
    "# train_data_path = \"./NER/Chinese/mytrain.txt\"\n",
    "train_data_path = \"./NER/English/mytrain.txt\"\n",
    "train_data = process_data(train_data_path)\n",
    "\n",
    "tags = train_data['tags']\n",
    "words = train_data['words']\n",
    "tag_seqs = train_data['tag_seqs']\n",
    "word_seqs = train_data['word_seqs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fcc02cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理批次 0-16/344 ...\n",
      "批次损失: 33.90\n",
      "正在处理批次 16-32/344 ...\n",
      "批次损失: 14.15\n",
      "正在处理批次 32-48/344 ...\n",
      "批次损失: 21.70\n",
      "正在处理批次 48-64/344 ...\n",
      "批次损失: 9.93\n",
      "正在处理批次 64-80/344 ...\n",
      "批次损失: 13.43\n",
      "正在处理批次 80-96/344 ...\n",
      "批次损失: 12.58\n",
      "正在处理批次 96-112/344 ...\n",
      "批次损失: 9.48\n",
      "正在处理批次 112-128/344 ...\n",
      "批次损失: 10.14\n",
      "正在处理批次 128-144/344 ...\n",
      "批次损失: 8.77\n",
      "正在处理批次 144-160/344 ...\n",
      "批次损失: 9.21\n",
      "正在处理批次 160-176/344 ...\n",
      "批次损失: 11.81\n",
      "正在处理批次 176-192/344 ...\n",
      "批次损失: 12.54\n",
      "正在处理批次 192-208/344 ...\n",
      "批次损失: 8.80\n",
      "正在处理批次 208-224/344 ...\n",
      "批次损失: 11.25\n",
      "正在处理批次 224-240/344 ...\n",
      "批次损失: 10.92\n",
      "正在处理批次 240-256/344 ...\n",
      "批次损失: 9.40\n",
      "正在处理批次 256-272/344 ...\n",
      "批次损失: 6.21\n",
      "正在处理批次 272-288/344 ...\n",
      "批次损失: 10.00\n",
      "正在处理批次 288-304/344 ...\n",
      "批次损失: 10.88\n",
      "正在处理批次 304-320/344 ...\n",
      "批次损失: 6.84\n",
      "正在处理批次 320-336/344 ...\n",
      "批次损失: 8.37\n",
      "正在处理批次 336-352/344 ...\n",
      "批次损失: 12.60\n",
      "第 0 次迭代, Loss=11.94\n",
      "--------------------------------------------------\n",
      "正在处理批次 0-16/344 ...\n",
      "批次损失: 9.88\n",
      "正在处理批次 16-32/344 ...\n",
      "批次损失: 8.38\n",
      "正在处理批次 32-48/344 ...\n",
      "批次损失: 11.38\n",
      "正在处理批次 48-64/344 ...\n",
      "批次损失: 8.38\n",
      "正在处理批次 64-80/344 ...\n",
      "批次损失: 11.34\n",
      "正在处理批次 80-96/344 ...\n",
      "批次损失: 11.13\n",
      "正在处理批次 96-112/344 ...\n",
      "批次损失: 8.23\n",
      "正在处理批次 112-128/344 ...\n",
      "批次损失: 8.42\n",
      "正在处理批次 128-144/344 ...\n",
      "批次损失: 7.62\n",
      "正在处理批次 144-160/344 ...\n",
      "批次损失: 7.94\n",
      "正在处理批次 160-176/344 ...\n",
      "批次损失: 10.67\n",
      "正在处理批次 176-192/344 ...\n",
      "批次损失: 10.99\n",
      "正在处理批次 192-208/344 ...\n",
      "批次损失: 8.42\n",
      "正在处理批次 208-224/344 ...\n",
      "批次损失: 10.88\n",
      "正在处理批次 224-240/344 ...\n",
      "批次损失: 9.88\n",
      "正在处理批次 240-256/344 ...\n",
      "批次损失: 8.24\n",
      "正在处理批次 256-272/344 ...\n",
      "批次损失: 5.90\n",
      "正在处理批次 272-288/344 ...\n",
      "批次损失: 9.65\n",
      "正在处理批次 288-304/344 ...\n",
      "批次损失: 10.07\n",
      "正在处理批次 304-320/344 ...\n",
      "批次损失: 6.23\n",
      "正在处理批次 320-336/344 ...\n",
      "批次损失: 7.81\n",
      "正在处理批次 336-352/344 ...\n",
      "批次损失: 11.02\n",
      "第 1 次迭代, Loss=9.16\n",
      "--------------------------------------------------\n",
      "正在处理批次 0-16/344 ...\n",
      "批次损失: 8.99\n",
      "正在处理批次 16-32/344 ...\n",
      "批次损失: 7.48\n",
      "正在处理批次 32-48/344 ...\n",
      "批次损失: 10.90\n",
      "正在处理批次 48-64/344 ...\n",
      "批次损失: 7.74\n",
      "正在处理批次 64-80/344 ...\n",
      "批次损失: 10.68\n",
      "正在处理批次 80-96/344 ...\n",
      "批次损失: 10.56\n",
      "正在处理批次 96-112/344 ...\n",
      "批次损失: 7.80\n",
      "正在处理批次 112-128/344 ...\n",
      "批次损失: 7.70\n",
      "正在处理批次 128-144/344 ...\n",
      "批次损失: 7.06\n",
      "正在处理批次 144-160/344 ...\n",
      "批次损失: 7.50\n",
      "正在处理批次 160-176/344 ...\n",
      "批次损失: 10.10\n",
      "正在处理批次 176-192/344 ...\n",
      "批次损失: 10.17\n",
      "正在处理批次 192-208/344 ...\n",
      "批次损失: 8.31\n",
      "正在处理批次 208-224/344 ...\n",
      "批次损失: 10.84\n",
      "正在处理批次 224-240/344 ...\n",
      "批次损失: 9.33\n",
      "正在处理批次 240-256/344 ...\n",
      "批次损失: 7.64\n",
      "正在处理批次 256-272/344 ...\n",
      "批次损失: 5.82\n",
      "正在处理批次 272-288/344 ...\n",
      "批次损失: 9.55\n",
      "正在处理批次 288-304/344 ...\n",
      "批次损失: 9.61\n",
      "正在处理批次 304-320/344 ...\n",
      "批次损失: 5.93\n",
      "正在处理批次 320-336/344 ...\n",
      "批次损失: 7.62\n",
      "正在处理批次 336-352/344 ...\n",
      "批次损失: 9.89\n",
      "第 2 次迭代, Loss=8.66\n",
      "--------------------------------------------------\n",
      "正在处理批次 0-16/344 ...\n",
      "批次损失: 8.53\n",
      "正在处理批次 16-32/344 ...\n",
      "批次损失: 6.92\n",
      "正在处理批次 32-48/344 ...\n",
      "批次损失: 10.64\n",
      "正在处理批次 48-64/344 ...\n",
      "批次损失: 7.35\n",
      "正在处理批次 64-80/344 ...\n",
      "批次损失: 10.34\n",
      "正在处理批次 80-96/344 ...\n",
      "批次损失: 10.29\n",
      "正在处理批次 96-112/344 ...\n",
      "批次损失: 7.59\n",
      "正在处理批次 112-128/344 ...\n",
      "批次损失: 7.32\n",
      "正在处理批次 128-144/344 ...\n",
      "批次损失: 6.73\n",
      "正在处理批次 144-160/344 ...\n",
      "批次损失: 7.39\n",
      "正在处理批次 160-176/344 ...\n",
      "批次损失: 9.72\n",
      "正在处理批次 176-192/344 ...\n",
      "批次损失: 9.65\n",
      "正在处理批次 192-208/344 ...\n",
      "批次损失: 8.28\n",
      "正在处理批次 208-224/344 ...\n",
      "批次损失: 10.89\n",
      "正在处理批次 224-240/344 ...\n",
      "批次损失: 9.00\n",
      "正在处理批次 240-256/344 ...\n",
      "批次损失: 7.27\n",
      "正在处理批次 256-272/344 ...\n",
      "批次损失: 5.81\n",
      "正在处理批次 272-288/344 ...\n",
      "批次损失: 9.52\n",
      "正在处理批次 288-304/344 ...\n",
      "批次损失: 9.36\n",
      "正在处理批次 304-320/344 ...\n",
      "批次损失: 5.78\n",
      "正在处理批次 320-336/344 ...\n",
      "批次损失: 7.55\n",
      "正在处理批次 336-352/344 ...\n",
      "批次损失: 9.04\n",
      "第 3 次迭代, Loss=8.39\n",
      "--------------------------------------------------\n",
      "正在处理批次 0-16/344 ...\n",
      "批次损失: 8.30\n",
      "正在处理批次 16-32/344 ...\n",
      "批次损失: 6.57\n",
      "正在处理批次 32-48/344 ...\n",
      "批次损失: 10.50\n",
      "正在处理批次 48-64/344 ...\n",
      "批次损失: 7.10\n",
      "正在处理批次 64-80/344 ...\n",
      "批次损失: 10.19\n",
      "正在处理批次 80-96/344 ...\n",
      "批次损失: 10.20\n",
      "正在处理批次 96-112/344 ...\n",
      "批次损失: 7.50\n",
      "正在处理批次 112-128/344 ...\n",
      "批次损失: 7.14\n",
      "正在处理批次 128-144/344 ...\n",
      "批次损失: 6.54\n",
      "正在处理批次 144-160/344 ...\n",
      "批次损失: 7.45\n",
      "正在处理批次 160-176/344 ...\n",
      "批次损失: 9.47\n",
      "正在处理批次 176-192/344 ...\n",
      "批次损失: 9.33\n",
      "正在处理批次 192-208/344 ...\n",
      "批次损失: 8.32\n",
      "正在处理批次 208-224/344 ...\n",
      "批次损失: 10.98\n",
      "正在处理批次 224-240/344 ...\n",
      "批次损失: 8.82\n",
      "正在处理批次 240-256/344 ...\n",
      "批次损失: 7.04\n",
      "正在处理批次 256-272/344 ...\n",
      "批次损失: 5.84\n",
      "正在处理批次 272-288/344 ...\n",
      "批次损失: 9.53\n",
      "正在处理批次 288-304/344 ...\n",
      "批次损失: 9.24\n",
      "正在处理批次 304-320/344 ...\n",
      "批次损失: 5.72\n",
      "正在处理批次 320-336/344 ...\n",
      "批次损失: 7.52\n",
      "正在处理批次 336-352/344 ...\n",
      "批次损失: 8.37\n",
      "第 4 次迭代, Loss=8.26\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 训练CRF模型\n",
    "crf = CRF(tags, feature_templates)\n",
    "crf.train(word_seqs, tag_seqs, batch_size=16, max_iter=5, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228556ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_validation_file(input_file, output_file, crf):\n",
    "    current_sentence = []\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as fin, open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            if line.strip() == \"\":\n",
    "                # 处理一个完整句子\n",
    "                if current_sentence:\n",
    "                    words = [word.lower() for word, _ in current_sentence]\n",
    "                    predicted_tags = crf.viterbi_decode(words)\n",
    "                    for (word, _), tag in zip(current_sentence, predicted_tags):\n",
    "                        fout.write(f\"{word} {tag}\\n\")\n",
    "                    fout.write(\"\\n\")\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                # 非空行，读取单词\n",
    "                parts = line.split()\n",
    "                word = parts[0]\n",
    "                current_sentence.append((word, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513df7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crf.save(\"model/crf_cn.pkl\")\n",
    "# crf.save(\"model/crf_en.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68c692f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-PER     0.2385    0.1661    0.1958      1842\n",
      "       I-PER     0.1935    0.1951    0.1943      1307\n",
      "       B-ORG     0.3429    0.0626    0.1059      1341\n",
      "       I-ORG     0.0452    0.0120    0.0189       751\n",
      "       B-LOC     0.4892    0.1475    0.2267      1837\n",
      "       I-LOC     0.0000    0.0000    0.0000       257\n",
      "      B-MISC     1.0000    0.0011    0.0022       922\n",
      "      I-MISC     0.0000    0.0000    0.0000       346\n",
      "\n",
      "   micro avg     0.2516    0.1076    0.1508      8603\n",
      "   macro avg     0.2887    0.0731    0.0930      8603\n",
      "weighted avg     0.3495    0.1076    0.1382      8603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 验证集\n",
    "valid_data_path = \"NER/English/validation.txt\"\n",
    "valid_data = process_data(valid_data_path)\n",
    "\n",
    "output_path = \"output/crf_validation_output.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for seq, tags in zip(valid_data['word_seqs'], valid_data['tag_seqs']):\n",
    "        predicted_tags = crf.viterbi_decode(seq)\n",
    "        for word, tag in zip(seq, predicted_tags):\n",
    "            fout.write(f\"{word} {tag}\\n\")\n",
    "        fout.write(\"\\n\")  # 句子间空行\n",
    "\n",
    "check(language = \"English\", gold_path=valid_data_path, my_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99961d0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tags' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m crf = CRF(\u001b[43mtags\u001b[49m, feature_templates)\n\u001b[32m      2\u001b[39m crf.load(\u001b[33m\"\u001b[39m\u001b[33mmodel/crf_en.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# crf.load(\"model/crf_cn.pkl\")\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'tags' is not defined"
     ]
    }
   ],
   "source": [
    "crf = CRF(tags, feature_templates)\n",
    "crf.load(\"model/crf_en.pkl\")\n",
    "# crf.load(\"model/crf_cn.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab5f2fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1 Score = 0.8247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-PER     0.4400    0.0068    0.0134      1617\n",
      "       I-PER     0.0000    0.0000    0.0000      1156\n",
      "       B-ORG     0.0294    0.0006    0.0012      1661\n",
      "       I-ORG     0.0000    0.0000    0.0000       835\n",
      "       B-LOC     0.0000    0.0000    0.0000      1668\n",
      "       I-LOC     0.0000    0.0000    0.0000       257\n",
      "      B-MISC     0.0000    0.0000    0.0000       702\n",
      "      I-MISC     0.0000    0.0000    0.0000       216\n",
      "\n",
      "   micro avg     0.1818    0.0015    0.0029      8112\n",
      "   macro avg     0.0587    0.0009    0.0018      8112\n",
      "weighted avg     0.0937    0.0015    0.0029      8112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 从文件加载CRF模型\n",
    "with open(\"model/crf_English.pkl\", \"rb\") as f:\n",
    "    crf = dill.load(f)\n",
    "\n",
    "process_validation_file(\"pj2_test/english_test.txt\", \"english_test_CRF.txt\", crf)\n",
    "check(language=\"English\", gold_path=\"pj2_test/english_test.txt\", my_path=\"english_test_CRF.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9361f677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-PER     0.2090    0.1583    0.1802      1617\n",
      "       I-PER     0.1600    0.1756    0.1674      1156\n",
      "       B-ORG     0.2812    0.0488    0.0831      1661\n",
      "       I-ORG     0.0355    0.0120    0.0179       835\n",
      "       B-LOC     0.4453    0.1463    0.2202      1668\n",
      "       I-LOC     0.0000    0.0000    0.0000       257\n",
      "      B-MISC     0.0000    0.0000    0.0000       702\n",
      "      I-MISC     0.0000    0.0000    0.0000       216\n",
      "\n",
      "   micro avg     0.2158    0.0979    0.1347      8112\n",
      "   macro avg     0.1414    0.0676    0.0836      8112\n",
      "weighted avg     0.2172    0.0979    0.1239      8112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试集\n",
    "# test_data_path = \"pj2_test/chinese_test.txt\"\n",
    "test_data_path = \"pj2_test/english_test.txt\"\n",
    "test_data = process_data(test_data_path)\n",
    "\n",
    "output_path = \"output/crf_test_output.txt\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for seq, tags in zip(test_data['word_seqs'], test_data['tag_seqs']):\n",
    "        predicted_tags = crf.viterbi_decode(seq)\n",
    "        for word, tag in zip(seq, predicted_tags):\n",
    "            fout.write(f\"{word} {tag}\\n\")\n",
    "        fout.write(\"\\n\")  # 句子间空行\n",
    "\n",
    "# check(language = \"Chinese\", gold_path=test_data_path, my_path=output_path)\n",
    "check(language = \"English\", gold_path=test_data_path, my_path=output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
